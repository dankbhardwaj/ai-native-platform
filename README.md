# ğŸš€ AI-Native Platform
## DevOps â†’ SRE â†’ AIOps â†’ MLOps â†’ LLMOps (Production-Style Architecture)

A fully GitOps-driven AI-native Kubernetes platform integrating:

- â˜¸ Kubernetes Infrastructure
- ğŸ”„ GitOps (FluxCD)
- ğŸ“Š Observability (Prometheus + Grafana + OpenTelemetry + Tempo)
- ğŸ›¡ï¸ SRE Practices (SLOs, Error Budgets, Autoscaling)
- ğŸ¤– AIOps (Anomaly Detection + Remediation)
- ğŸ§  MLOps (Model Training & Serving)
- ğŸ”¥ LLMOps (LLM Telemetry + Token Monitoring)
- â˜ AWS Deployment

This project demonstrates how modern AI workloads should be operated in production environments.

---

# ğŸ— High-Level Architecture

Infrastructure â†’ GitOps â†’ Platform Services â†’ Applications â†’ Observability â†’ AI Automation

All deployments are Git-driven and automatically reconciled.

# ğŸ–¼ Architecture Diagram

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        Git Repository       â”‚
            â”‚  (GitOps Source of Truth)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    FluxCD    â”‚
                    â”‚ Reconciler   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚      Kubernetes Cluster   â”‚
             â”‚                           â”‚
             â”‚  Apps   |   AI   |  LLM   â”‚
             â”‚                           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Observability Stack             â”‚
        â”‚ Prometheus | Grafana | Tempo    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ AIOps & MLOps Engine  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

# ğŸ“‚ Repository Structure

## ğŸ“¦ Repository Structure

<img width="1536" height="1024" alt="ChatGPT Image Feb 14, 2026, 12_57_44 PM" src="https://github.com/user-attachments/assets/6c001689-36fa-4b65-98ac-3c123a45c826" />



---

# â˜¸ Kubernetes Cluster State

All services deployed declaratively via GitOps.

![Kubernetes Pods](kubernetes-pods.png)

---

# ğŸ”„ GitOps Continuous Delivery (FluxCD)

Git is the single source of truth.

- Any commit â†’ Automatically deployed
- Drift detection enabled
- Self-healing workloads

![Flux Status](flux-status.png)

---

# ğŸ“Š Observability Stack

## Prometheus Targets

All workloads scraped using ServiceMonitor.

![Prometheus Targets](prometheus-targets.png)

---

## LLM Telemetry Metrics

Tracked metrics:

- llm_requests_total
- llm_tokens_total
- llm_request_latency_seconds

![Prometheus LLM Graph](prometheus-llm-graph.png)

---

# ğŸ“ˆ Grafana Dashboard

Custom dashboard visualizing:

- Request Rate
- Token Consumption
- Latency Distribution
- Node Resource Metrics

![Grafana Dashboard](grafana-dashboard.png)

---

# ğŸ” Distributed Tracing (OpenTelemetry + Tempo)

Tracing enabled through OTEL Collector and Tempo.

- End-to-end trace visibility
- Latency analysis
- Request flow inspection

![Tempo Tracing](tempo-tracing.png)

---

# ğŸ“¡ LLM Metrics (Inside Pod)

Metrics exposed via `/metrics` endpoint.

![LLM Metrics](llm-metrics.png)

---

# ğŸ¤– AIOps Layer

## Model Training

Trigger:

POST /train


![AIOps Train](aiops-train.png)

---

## Anomaly Detection

Trigger:

POST /detect


Example output:

![AIOps Detect](aiops-detect.png)

This enables automated anomaly detection for production workloads.

---

# ğŸ›¡ï¸ SRE Layer

Located in:

sre/
â”œâ”€â”€ slos/
â”œâ”€â”€ error-budgets/
â”œâ”€â”€ autoscaling/
â””â”€â”€ chaos/


Implements:

- Service Level Objectives
- Error Budget policies
- Horizontal Pod Autoscaler
- Chaos engineering scenarios

Ensures system reliability under load.

---

# ğŸ§  MLOps Layer

mlops/
â”œâ”€â”€ training/
â”œâ”€â”€ pipelines/
â”œâ”€â”€ model-registry/
â””â”€â”€ serving/


Capabilities:

- Automated retraining
- Model validation
- Versioned deployments
- Metrics-driven rollback

---

# ğŸ”¥ LLMOps Layer

llmops/
â”œâ”€â”€ rag/
â”œâ”€â”€ prompt-versioning/
â”œâ”€â”€ vector-db/
â””â”€â”€ llm-serving/


Features:

- Token tracking
- Prompt version management
- LLM latency monitoring
- Production telemetry for generative AI

---

# ğŸ” CI/CD

ci/
â”œâ”€â”€ github-actions/
â””â”€â”€ security-scans/


Includes:

- Docker build pipeline
- Security image scanning
- GitOps deployment automation

---

# ğŸš€ Deployment Flow

1. Provision infrastructure (Terraform)
2. Bootstrap Kubernetes cluster
3. Install FluxCD
4. Push manifests to Git
5. Flux reconciles workloads
6. Prometheus scrapes metrics
7. Grafana visualizes telemetry
8. AIOps detects anomalies
9. MLOps retrains model
10. LLMOps monitors generative workloads

---

# ğŸ¯ Production Capabilities Demonstrated

âœ… GitOps-driven infrastructure  
âœ… Full observability pipeline  
âœ… AI-powered anomaly detection  
âœ… ML lifecycle automation  
âœ… LLM telemetry tracking  
âœ… Kubernetes production architecture  

---

# ğŸ›  Technology Stack

- Kubernetes
- FluxCD
- Prometheus
- Grafana
- OpenTelemetry
- Tempo
- FastAPI
- Python
- Docker
- Terraform
- AWS

---

# ğŸ‘¤ Author

Bhaskar Sharma  
DevOps | SRE | MLOps | AI Infrastructure Engineer  

---

