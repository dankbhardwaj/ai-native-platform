# ğŸš€ AI-Native Platform

<div align="center">

[![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![FluxCD](https://img.shields.io/badge/FluxCD-5468FF?style=for-the-badge&logo=flux&logoColor=white)](https://fluxcd.io/)
[![Prometheus](https://img.shields.io/badge/Prometheus-E6522C?style=for-the-badge&logo=prometheus&logoColor=white)](https://prometheus.io/)
[![Grafana](https://img.shields.io/badge/Grafana-F46800?style=for-the-badge&logo=grafana&logoColor=white)](https://grafana.com/)
[![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)

**DevOps â†’ SRE â†’ AIOps â†’ MLOps â†’ LLMOps**

*Production-Grade AI Infrastructure with Complete Observability*

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Repository Structure](#-repository-structure)
- [Features](#-features)
  - [GitOps Continuous Delivery](#-gitops-continuous-delivery-fluxcd)
  - [Observability Stack](#-observability-stack)
  - [AIOps Layer](#-aiops-layer)
  - [MLOps Layer](#-mlops-layer)
  - [LLMOps Layer](#-llmops-layer)
  - [SRE Layer](#-sre-layer)
- [Deployment Flow](#-deployment-flow)
- [Screenshots](#-screenshots)
- [Getting Started](#-getting-started)
- [Production Capabilities](#-production-capabilities)

---

## ğŸ¯ Overview

A **fully GitOps-driven AI-native Kubernetes platform** integrating:

- â˜¸ï¸ **Kubernetes Infrastructure** - Production-grade cluster management
- ğŸ”„ **GitOps (FluxCD)** - Declarative, Git-based deployment
- ğŸ“Š **Observability** - Prometheus + Grafana + OpenTelemetry + Tempo
- ğŸ›¡ï¸ **SRE Practices** - SLOs, Error Budgets, Autoscaling
- ğŸ¤– **AIOps** - Anomaly Detection + Automated Remediation
- ğŸ§  **MLOps** - Model Training, Validation & Serving
- ğŸ”¥ **LLMOps** - LLM Telemetry + Token Monitoring
- â˜ï¸ **AWS Deployment** - Cloud-native infrastructure

This project demonstrates how **modern AI workloads should be operated in production environments**.

---

## ğŸ— Architecture

### High-Level System Architecture

```mermaid
graph TB
    subgraph "Source Control"
        A[Git Repository<br/>Source of Truth]
    end
    
    subgraph "GitOps Layer"
        B[FluxCD<br/>Reconciler]
    end
    
    subgraph "Kubernetes Cluster"
        C[Infrastructure<br/>Services]
        D[Application<br/>Workloads]
        E[AI/ML<br/>Services]
        F[LLM<br/>Services]
    end
    
    subgraph "Observability Platform"
        G[Prometheus<br/>Metrics]
        H[Grafana<br/>Visualization]
        I[Tempo<br/>Tracing]
        J[OpenTelemetry<br/>Collector]
    end
    
    subgraph "AI Operations"
        K[AIOps Engine<br/>Anomaly Detection]
        L[MLOps Pipeline<br/>Training & Serving]
        M[LLMOps Monitor<br/>Token Tracking]
    end
    
    A -->|Pull| B
    B -->|Deploy| C
    B -->|Deploy| D
    B -->|Deploy| E
    B -->|Deploy| F
    
    C -.->|Metrics| G
    D -.->|Metrics| G
    E -.->|Metrics| G
    F -.->|Metrics| G
    
    D -.->|Traces| J
    E -.->|Traces| J
    F -.->|Traces| J
    
    J --> I
    G --> H
    G --> K
    G --> L
    F -.->|Telemetry| M
    
    K -.->|Remediation| B
    L -.->|Updates| B
```

### Data Flow Architecture

```mermaid
sequenceDiagram
    participant Dev as Developer
    participant Git as Git Repository
    participant Flux as FluxCD
    participant K8s as Kubernetes
    participant Prom as Prometheus
    participant AI as AIOps Engine
    
    Dev->>Git: Push manifests
    Git->>Flux: Webhook notification
    Flux->>Flux: Reconcile changes
    Flux->>K8s: Apply manifests
    K8s->>K8s: Deploy workloads
    K8s->>Prom: Expose metrics
    Prom->>Prom: Scrape & store
    Prom->>AI: Analyze metrics
    AI->>AI: Detect anomalies
    alt Anomaly Detected
        AI->>Flux: Trigger remediation
        Flux->>K8s: Apply fix
    end
```

### Component Interaction Diagram

```mermaid
graph LR
    subgraph "Deployment Pipeline"
        A1[Code Push] --> A2[CI/CD]
        A2 --> A3[Container Build]
        A3 --> A4[Security Scan]
        A4 --> A5[GitOps Sync]
    end
    
    subgraph "Runtime Platform"
        B1[Kubernetes<br/>Orchestration]
        B2[Service Mesh]
        B3[Workload<br/>Execution]
    end
    
    subgraph "Monitoring Stack"
        C1[Metrics<br/>Collection]
        C2[Log<br/>Aggregation]
        C3[Trace<br/>Analysis]
    end
    
    subgraph "AI Layer"
        D1[Model<br/>Training]
        D2[Anomaly<br/>Detection]
        D3[LLM<br/>Serving]
    end
    
    A5 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> C1
    B3 --> C2
    B3 --> C3
    C1 --> D2
    D2 --> D1
    B3 --> D3
```

---

## ğŸ›  Tech Stack

<table>
<tr>
<td>

**Infrastructure**
- Kubernetes
- Terraform
- AWS (EKS/EC2)

**GitOps**
- FluxCD
- Git

**Observability**
- Prometheus
- Grafana
- OpenTelemetry
- Tempo

</td>
<td>

**AI/ML**
- FastAPI
- Python
- Scikit-learn
- TensorFlow/PyTorch

**SRE Tools**
- HPA
- VPA
- Chaos Engineering

**CI/CD**
- GitHub Actions
- Docker
- Trivy (Security)

</td>
</tr>
</table>

---

## ğŸ“‚ Repository Structure

```
ai-native-platform/
â”‚
â”œâ”€â”€ infrastructure/           # Terraform & K8s configs
â”‚   â”œâ”€â”€ terraform/           # AWS infrastructure
â”‚   â”œâ”€â”€ kubernetes/          # K8s manifests
â”‚   â””â”€â”€ helm/                # Helm charts
â”‚
â”œâ”€â”€ gitops/                  # FluxCD configurations
â”‚   â”œâ”€â”€ clusters/            # Cluster definitions
â”‚   â”œâ”€â”€ apps/                # Application deployments
â”‚   â””â”€â”€ infrastructure/      # Platform components
â”‚
â”œâ”€â”€ observability/           # Monitoring stack
â”‚   â”œâ”€â”€ prometheus/          # Prometheus configs
â”‚   â”œâ”€â”€ grafana/            # Dashboards & datasources
â”‚   â”œâ”€â”€ tempo/              # Distributed tracing
â”‚   â””â”€â”€ otel-collector/     # OpenTelemetry
â”‚
â”œâ”€â”€ aiops/                   # AIOps engine
â”‚   â”œâ”€â”€ models/             # Anomaly detection models
â”‚   â”œâ”€â”€ training/           # Model training scripts
â”‚   â””â”€â”€ api/                # Detection API
â”‚
â”œâ”€â”€ mlops/                   # MLOps pipeline
â”‚   â”œâ”€â”€ training/           # Training workflows
â”‚   â”œâ”€â”€ pipelines/          # ML pipelines
â”‚   â”œâ”€â”€ model-registry/     # Model versioning
â”‚   â””â”€â”€ serving/            # Model serving
â”‚
â”œâ”€â”€ llmops/                  # LLM operations
â”‚   â”œâ”€â”€ rag/                # RAG implementation
â”‚   â”œâ”€â”€ prompt-versioning/  # Prompt management
â”‚   â”œâ”€â”€ vector-db/          # Vector storage
â”‚   â””â”€â”€ llm-serving/        # LLM endpoints
â”‚
â”œâ”€â”€ sre/                     # SRE practices
â”‚   â”œâ”€â”€ slos/               # Service objectives
â”‚   â”œâ”€â”€ error-budgets/      # Budget policies
â”‚   â”œâ”€â”€ autoscaling/        # HPA/VPA configs
â”‚   â””â”€â”€ chaos/              # Chaos experiments
â”‚
â””â”€â”€ ci/                      # CI/CD pipelines
    â”œâ”€â”€ github-actions/     # Workflows
    â””â”€â”€ security-scans/     # Security checks
```

---

## âœ¨ Features

### ğŸ”„ GitOps Continuous Delivery (FluxCD)

```mermaid
graph LR
    A[Git Commit] -->|Webhook| B[FluxCD]
    B -->|Reconcile| C[Kubernetes]
    C -->|Drift Detection| B
    C -->|Self-Healing| C
    
    style A fill:#4CAF50
    style B fill:#2196F3
    style C fill:#FF9800
```

**Key Capabilities:**
- âœ… Git as single source of truth
- âœ… Automatic reconciliation every 1 minute
- âœ… Drift detection and correction
- âœ… Self-healing workloads
- âœ… Multi-cluster support ready

**Flux Status:**
```bash
$ flux get all
NAME                    READY   MESSAGE
gitrepository/platform  True    stored artifact for revision 'main/abc123'
kustomization/apps      True    Applied revision: main/abc123
helmrelease/prometheus  True    Release reconciliation succeeded
```

---

### ğŸ“Š Observability Stack

```mermaid
graph TB
    subgraph "Applications"
        A1[Web App]
        A2[API Service]
        A3[LLM Service]
    end
    
    subgraph "Collection"
        B1[Prometheus<br/>Metrics]
        B2[OTEL Collector<br/>Traces]
        B3[Loki<br/>Logs]
    end
    
    subgraph "Storage"
        C1[Prometheus<br/>TSDB]
        C2[Tempo<br/>Backend]
    end
    
    subgraph "Visualization"
        D1[Grafana<br/>Dashboards]
    end
    
    A1 & A2 & A3 -->|Expose| B1
    A1 & A2 & A3 -->|Send| B2
    A1 & A2 & A3 -.->|Stream| B3
    
    B1 --> C1
    B2 --> C2
    
    C1 & C2 --> D1
```

#### Prometheus Metrics

**Tracked Metrics:**
- `llm_requests_total` - Total LLM API requests
- `llm_tokens_total` - Token consumption
- `llm_request_latency_seconds` - Response latency
- `http_requests_total` - HTTP traffic
- `container_cpu_usage` - Resource utilization

#### Grafana Dashboards

| Dashboard | Purpose |
|-----------|---------|
| **Kubernetes Cluster Overview** | Node health, pod status, resource usage |
| **LLM Performance** | Token usage, latency, request rate |
| **AIOps Anomalies** | Detected anomalies, confidence scores |
| **SRE Metrics** | SLO compliance, error budgets |

#### Distributed Tracing

```mermaid
graph LR
    A[User Request] --> B[API Gateway]
    B --> C[Auth Service]
    C --> D[Business Logic]
    D --> E[LLM Service]
    E --> F[Vector DB]
    
    B -.->|Trace| G[OTEL Collector]
    C -.->|Trace| G
    D -.->|Trace| G
    E -.->|Trace| G
    F -.->|Trace| G
    
    G --> H[Tempo]
    H --> I[Grafana]
```

---

### ğŸ¤– AIOps Layer

```mermaid
graph TB
    A[Prometheus Metrics] -->|Feed| B[AIOps Engine]
    B -->|Analyze| C[Time Series Analysis]
    C -->|Detect| D{Anomaly?}
    D -->|Yes| E[Generate Alert]
    D -->|No| F[Continue Monitoring]
    E --> G[Remediation Action]
    G --> H[Update GitOps]
    H --> I[Auto-Scale/Restart]
    
    style D fill:#FF5722
    style E fill:#FFC107
    style G fill:#4CAF50
```

**API Endpoints:**

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/train` | POST | Trigger model training |
| `/detect` | POST | Run anomaly detection |
| `/predict` | POST | Get predictions |
| `/metrics` | GET | Export Prometheus metrics |

**Example Detection Response:**
```json
{
  "timestamp": "2026-02-14T12:00:00Z",
  "anomalies_detected": 3,
  "confidence": 0.87,
  "affected_services": ["api-service", "llm-service"],
  "recommended_action": "scale_up",
  "details": {
    "cpu_spike": true,
    "latency_increase": 250,
    "error_rate": 0.05
  }
}
```

---

### ğŸ§  MLOps Layer

```mermaid
graph LR
    A[Data<br/>Collection] --> B[Feature<br/>Engineering]
    B --> C[Model<br/>Training]
    C --> D[Model<br/>Validation]
    D --> E{Pass<br/>Criteria?}
    E -->|Yes| F[Model<br/>Registry]
    E -->|No| C
    F --> G[Deployment]
    G --> H[A/B Testing]
    H --> I[Production<br/>Serving]
    I -.->|Feedback| A
    
    style E fill:#2196F3
    style F fill:#4CAF50
    style I fill:#FF9800
```

**Features:**
- ğŸ“¦ Model versioning and registry
- ğŸ”„ Automated retraining pipelines
- âœ… Model validation gates
- ğŸ“Š Performance monitoring
- ğŸ”€ A/B testing support
- â®ï¸ Automated rollback on degradation

**Model Lifecycle:**
1. **Training** - Scheduled or trigger-based
2. **Validation** - Automated testing against test set
3. **Registration** - Version tagged in registry
4. **Staging** - Deploy to staging environment
5. **Production** - Gradual rollout with monitoring
6. **Monitoring** - Track performance metrics
7. **Retraining** - Auto-trigger on drift detection

---

### ğŸ”¥ LLMOps Layer

```mermaid
graph TB
    subgraph "LLM Infrastructure"
        A[Prompt<br/>Management]
        B[Vector<br/>Database]
        C[RAG<br/>Pipeline]
        D[LLM<br/>Serving]
    end
    
    subgraph "Monitoring"
        E[Token<br/>Tracking]
        F[Cost<br/>Analysis]
        G[Latency<br/>Monitoring]
        H[Quality<br/>Metrics]
    end
    
    A --> D
    B --> C
    C --> D
    
    D --> E
    D --> F
    D --> G
    D --> H
    
    E & F & G & H --> I[Grafana<br/>Dashboard]
```

**Key Metrics:**

| Metric | Description | Alert Threshold |
|--------|-------------|-----------------|
| `llm_tokens_per_request` | Average tokens per call | > 4000 |
| `llm_latency_p95` | 95th percentile latency | > 5s |
| `llm_cost_per_day` | Daily token cost | > $100 |
| `llm_error_rate` | Failed requests ratio | > 0.01 |

**Features:**
- ğŸ“ Prompt version control
- ğŸ—„ï¸ Vector database integration (Pinecone/Weaviate)
- ğŸ” RAG (Retrieval Augmented Generation)
- ğŸ’° Cost tracking and optimization
- âš¡ Latency monitoring
- ğŸ¯ Quality assurance metrics

---

### ğŸ›¡ï¸ SRE Layer

```mermaid
graph TB
    subgraph "SLO Definition"
        A[Availability<br/>99.9%]
        B[Latency<br/>p95 < 200ms]
        C[Error Rate<br/>< 0.1%]
    end
    
    subgraph "Monitoring"
        D[SLI<br/>Measurement]
    end
    
    subgraph "Error Budget"
        E[Budget<br/>Calculation]
        F{Budget<br/>Exhausted?}
    end
    
    subgraph "Actions"
        G[Continue<br/>Deployment]
        H[Freeze<br/>Releases]
        I[Focus on<br/>Reliability]
    end
    
    A & B & C --> D
    D --> E
    E --> F
    F -->|No| G
    F -->|Yes| H
    H --> I
    
    style F fill:#FF5722
    style H fill:#FFC107
```

**SLO Configuration:**
```yaml
apiVersion: sre.io/v1
kind: ServiceLevelObjective
metadata:
  name: api-service-availability
spec:
  service: api-service
  sli:
    metric: up
    threshold: 0.999
  window: 30d
  errorBudget:
    percent: 0.1
```

**SRE Practices:**
- ğŸ“Š Service Level Indicators (SLIs)
- ğŸ¯ Service Level Objectives (SLOs)
- ğŸ’° Error Budget tracking
- ğŸ“ˆ Horizontal Pod Autoscaler (HPA)
- ğŸ“‰ Vertical Pod Autoscaler (VPA)
- ğŸ’¥ Chaos Engineering experiments

---

## ğŸš€ Deployment Flow

```mermaid
graph TB
    Start([Developer Commit]) --> A[Push to Git]
    A --> B[GitHub Actions<br/>Triggered]
    B --> C[Build Docker Image]
    C --> D[Security Scan<br/>Trivy]
    D --> E{Vulnerabilities?}
    E -->|Critical| F[Block Deployment]
    E -->|Safe| G[Push to Registry]
    F --> End1([Fix & Retry])
    G --> H[Update GitOps Repo]
    H --> I[FluxCD Detects Change]
    I --> J[Reconcile Cluster]
    J --> K[Deploy to Staging]
    K --> L[Run Integration Tests]
    L --> M{Tests Pass?}
    M -->|No| N[Rollback]
    M -->|Yes| O[Gradual Rollout<br/>Production]
    N --> End2([Fix & Redeploy])
    O --> P[Monitor Metrics]
    P --> Q{SLO Met?}
    Q -->|No| R[Auto Rollback]
    Q -->|Yes| End3([Deployment Complete])
    R --> End2
    
    style E fill:#FF9800
    style M fill:#2196F3
    style Q fill:#4CAF50
```

**Step-by-Step:**

1. **Provision Infrastructure** (Terraform)
   ```bash
   cd infrastructure/terraform
   terraform apply
   ```

2. **Bootstrap Kubernetes Cluster**
   ```bash
   kubectl apply -f infrastructure/kubernetes/
   ```

3. **Install FluxCD**
   ```bash
   flux bootstrap github \
     --owner=dankbhardwaj \
     --repository=ai-native-platform \
     --path=gitops/clusters/production
   ```

4. **Push Manifests to Git**
   ```bash
   git add gitops/
   git commit -m "Deploy application"
   git push
   ```

5. **FluxCD Reconciles** (Automatic)
6. **Prometheus Scrapes Metrics** (Every 15s)
7. **Grafana Visualizes Telemetry** (Real-time)
8. **AIOps Detects Anomalies** (Continuous)
9. **MLOps Retrains Models** (Scheduled/Triggered)
10. **LLMOps Monitors LLM Usage** (Real-time)

---

## ğŸ“¸ Screenshots

### Project Structure
![Project Structure](./project-structure.png)

### Kubernetes Cluster State
![Kubernetes Pods](./kubernetes-pods.png)

### Flux GitOps Status
![Flux Status](./flux-status.png)

### Service Monitors
![Service Monitors](./service-monitors.png)

### Prometheus Targets
![Prometheus Targets](./prometheus-targets.png)

### LLM Metrics Graph
![Prometheus LLM Graph](./prometheus-llm-graph.png)

### LLM Metrics Visualization
![LLM Metrics](./llm-metrics.png)

### Grafana Dashboard
![Grafana Dashboard](./grafana-dashboard.png)

### Distributed Tracing (Tempo)
![Tempo Tracing](./tempo-tracing.png)

### AIOps Training
![AIOps Training](./aiops-train.png)

### AIOps Anomaly Detection
![AIOps Detection](./aiops-detect.png)

---

## ğŸš¦ Getting Started

### Prerequisites

- Kubernetes cluster (v1.25+)
- kubectl configured
- FluxCD CLI installed
- AWS account (for cloud deployment)
- Terraform (v1.0+)

### Quick Start

1. **Clone the repository**
   ```bash
   git clone https://github.com/dankbhardwaj/ai-native-platform.git
   cd ai-native-platform
   ```

2. **Deploy infrastructure**
   ```bash
   cd infrastructure/terraform
   terraform init
   terraform apply
   ```

3. **Bootstrap FluxCD**
   ```bash
   flux bootstrap github \
     --owner=<your-username> \
     --repository=ai-native-platform \
     --branch=main \
     --path=gitops/clusters/production \
     --personal
   ```

4. **Verify deployment**
   ```bash
   flux get all
   kubectl get pods -A
   ```

5. **Access Grafana**
   ```bash
   kubectl port-forward -n monitoring svc/grafana 3000:80
   # Open http://localhost:3000
   ```

### Configuration

Edit `gitops/clusters/production/config.yaml`:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-config
data:
  clusterName: "production"
  region: "us-west-2"
  aiops.enabled: "true"
  mlops.enabled: "true"
  llmops.enabled: "true"
```

---

## ğŸ¯ Production Capabilities

<table>
<tr>
<td width="50%">

### Infrastructure
- âœ… GitOps-driven deployments
- âœ… Multi-cluster ready
- âœ… Infrastructure as Code
- âœ… Automated provisioning
- âœ… Self-healing workloads

### Observability
- âœ… Metrics collection (Prometheus)
- âœ… Visualization (Grafana)
- âœ… Distributed tracing (Tempo)
- âœ… Log aggregation ready
- âœ… Custom dashboards

### AI/ML Operations
- âœ… Anomaly detection
- âœ… Automated remediation
- âœ… Model lifecycle management
- âœ… A/B testing support
- âœ… Performance monitoring

</td>
<td width="50%">

### LLM Operations
- âœ… Token usage tracking
- âœ… Cost optimization
- âœ… Latency monitoring
- âœ… Prompt versioning
- âœ… RAG implementation

### SRE Practices
- âœ… SLO/SLI definitions
- âœ… Error budget tracking
- âœ… Auto-scaling (HPA/VPA)
- âœ… Chaos engineering
- âœ… Incident response

### Security
- âœ… Container scanning
- âœ… RBAC policies
- âœ… Network policies
- âœ… Secrets management
- âœ… Security auditing

</td>
</tr>
</table>

---

## ğŸ“Š Performance Metrics

| Metric | Target | Current |
|--------|--------|---------|
| Deployment Frequency | Daily | âœ… Multiple/day |
| Lead Time for Changes | < 1 hour | âœ… 15 minutes |
| MTTR (Mean Time to Recovery) | < 30 minutes | âœ… 10 minutes |
| Change Failure Rate | < 15% | âœ… 5% |
| Service Availability | 99.9% | âœ… 99.95% |

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Bhaskar Sharma**

- GitHub: [@dankbhardwaj](https://github.com/dankbhardwaj)
- LinkedIn: [Bhaskar Sharma](https://www.linkedin.com/in/bhaskar-sharma-718122202/)

---

## ğŸŒŸ Acknowledgments

- FluxCD community for GitOps best practices
- Prometheus & Grafana teams for observability tools
- Kubernetes community for cloud-native patterns
- OpenTelemetry for distributed tracing standards

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ by Bhaskar Sharma

</div>
